[
    {
        "title": "Web Crawler Multithreaded",
        "question_content": null,
        "solutions": [],
        "discussions": [
            {
                "id": 1565242,
                "content": [
                    {
                        "username": "mshlis",
                        "content": "Follow up:\\n\\n1) Assume we have 10,000 nodes and 1 billion URLs to crawl. We will deploy the same software onto each node. The software can know about all the nodes. We have to minimize communication between machines and make sure each node does equal amount of work. How would your web crawler design change?\\n\\n2) What if one node fails or does not work?\\n\\n3) How do you know when the crawler is done?"
                    },
                    {
                        "username": "Elimathew72",
                        "content": "Shoudn\\'t \"http://psn.wlyby.edu/upkr\" and \"http://psn.wlyby.edu/ubmr be under the same host ?"
                    },
                    {
                        "username": "edaengineer",
                        "content": "All other concurrency questions are much easier compared to this. Why is it not HARD?"
                    },
                    {
                        "username": "richardtang",
                        "content": "Not sure why this solution is always time limited exceeded on 18/20\\n ```\\n# \"\"\"\\n# This is HtmlParser\\'s API interface.\\n# You should not implement it, or speculate about its implementation\\n# \"\"\"\\n#class HtmlParser(object):\\n#    def getUrls(self, url):\\n#        \"\"\"\\n#        :type url: str\\n#        :rtype List[str]\\n#        \"\"\"\\nimport threading\\nfrom collections import deque\\n\\nclass Solution:\\n    def crawl(self, startUrl: str, htmlParser: \\'HtmlParser\\') -> List[str]:\\n        q = deque([startUrl])\\n        visited = {startUrl}\\n\\n        def hostname(url):\\n            return url.split(\\'/\\')[2]\\n\\n        name = hostname(startUrl)\\n        lock = threading.Lock()\\n\\n        def worker():\\n            while q:\\n                    lock.acquire()\\n                    url = q.popleft()\\n                    lock.release()\\n                    urls = htmlParser.getUrls(url)\\n                    for u in urls:\\n                        if u not in visited and hostname(u)==name:\\n                            lock.acquire()\\n                            if u not in visited:\\n                                q.append(u)\\n                                visited.add(u)\\n                            lock.release()\\n\\n        threads = [threading.Thread(target=worker) for _ in range(8)]\\n        for t in threads:\\n            t.start()\\n\\n        for t in threads:\\n            t.join()\\n        return list(visited)           \\n```"
                    },
                    {
                        "username": "lion2012",
                        "content": "I could not understand the need of input vairable  edges = [[2,0],[2,1],[3,2],[3,1],[0,4]] . \\n\\n\\n\\n![image](https://assets.leetcode.com/users/images/93608c17-d51f-4e25-9fd3-f988de38eac7_1618666015.6092336.png)\\n\\n\\n\\n\\nWhat does edge [2.0] even means in the above image?\\n\\nCan someone plesae explain me its role in the solution. \\n\\nThanks"
                    }
                ]
            },
            {
                "id": 1569756,
                "content": [
                    {
                        "username": "mshlis",
                        "content": "Follow up:\\n\\n1) Assume we have 10,000 nodes and 1 billion URLs to crawl. We will deploy the same software onto each node. The software can know about all the nodes. We have to minimize communication between machines and make sure each node does equal amount of work. How would your web crawler design change?\\n\\n2) What if one node fails or does not work?\\n\\n3) How do you know when the crawler is done?"
                    },
                    {
                        "username": "Elimathew72",
                        "content": "Shoudn\\'t \"http://psn.wlyby.edu/upkr\" and \"http://psn.wlyby.edu/ubmr be under the same host ?"
                    },
                    {
                        "username": "edaengineer",
                        "content": "All other concurrency questions are much easier compared to this. Why is it not HARD?"
                    },
                    {
                        "username": "richardtang",
                        "content": "Not sure why this solution is always time limited exceeded on 18/20\\n ```\\n# \"\"\"\\n# This is HtmlParser\\'s API interface.\\n# You should not implement it, or speculate about its implementation\\n# \"\"\"\\n#class HtmlParser(object):\\n#    def getUrls(self, url):\\n#        \"\"\"\\n#        :type url: str\\n#        :rtype List[str]\\n#        \"\"\"\\nimport threading\\nfrom collections import deque\\n\\nclass Solution:\\n    def crawl(self, startUrl: str, htmlParser: \\'HtmlParser\\') -> List[str]:\\n        q = deque([startUrl])\\n        visited = {startUrl}\\n\\n        def hostname(url):\\n            return url.split(\\'/\\')[2]\\n\\n        name = hostname(startUrl)\\n        lock = threading.Lock()\\n\\n        def worker():\\n            while q:\\n                    lock.acquire()\\n                    url = q.popleft()\\n                    lock.release()\\n                    urls = htmlParser.getUrls(url)\\n                    for u in urls:\\n                        if u not in visited and hostname(u)==name:\\n                            lock.acquire()\\n                            if u not in visited:\\n                                q.append(u)\\n                                visited.add(u)\\n                            lock.release()\\n\\n        threads = [threading.Thread(target=worker) for _ in range(8)]\\n        for t in threads:\\n            t.start()\\n\\n        for t in threads:\\n            t.join()\\n        return list(visited)           \\n```"
                    },
                    {
                        "username": "lion2012",
                        "content": "I could not understand the need of input vairable  edges = [[2,0],[2,1],[3,2],[3,1],[0,4]] . \\n\\n\\n\\n![image](https://assets.leetcode.com/users/images/93608c17-d51f-4e25-9fd3-f988de38eac7_1618666015.6092336.png)\\n\\n\\n\\n\\nWhat does edge [2.0] even means in the above image?\\n\\nCan someone plesae explain me its role in the solution. \\n\\nThanks"
                    }
                ]
            },
            {
                "id": 1568302,
                "content": [
                    {
                        "username": "mshlis",
                        "content": "Follow up:\\n\\n1) Assume we have 10,000 nodes and 1 billion URLs to crawl. We will deploy the same software onto each node. The software can know about all the nodes. We have to minimize communication between machines and make sure each node does equal amount of work. How would your web crawler design change?\\n\\n2) What if one node fails or does not work?\\n\\n3) How do you know when the crawler is done?"
                    },
                    {
                        "username": "Elimathew72",
                        "content": "Shoudn\\'t \"http://psn.wlyby.edu/upkr\" and \"http://psn.wlyby.edu/ubmr be under the same host ?"
                    },
                    {
                        "username": "edaengineer",
                        "content": "All other concurrency questions are much easier compared to this. Why is it not HARD?"
                    },
                    {
                        "username": "richardtang",
                        "content": "Not sure why this solution is always time limited exceeded on 18/20\\n ```\\n# \"\"\"\\n# This is HtmlParser\\'s API interface.\\n# You should not implement it, or speculate about its implementation\\n# \"\"\"\\n#class HtmlParser(object):\\n#    def getUrls(self, url):\\n#        \"\"\"\\n#        :type url: str\\n#        :rtype List[str]\\n#        \"\"\"\\nimport threading\\nfrom collections import deque\\n\\nclass Solution:\\n    def crawl(self, startUrl: str, htmlParser: \\'HtmlParser\\') -> List[str]:\\n        q = deque([startUrl])\\n        visited = {startUrl}\\n\\n        def hostname(url):\\n            return url.split(\\'/\\')[2]\\n\\n        name = hostname(startUrl)\\n        lock = threading.Lock()\\n\\n        def worker():\\n            while q:\\n                    lock.acquire()\\n                    url = q.popleft()\\n                    lock.release()\\n                    urls = htmlParser.getUrls(url)\\n                    for u in urls:\\n                        if u not in visited and hostname(u)==name:\\n                            lock.acquire()\\n                            if u not in visited:\\n                                q.append(u)\\n                                visited.add(u)\\n                            lock.release()\\n\\n        threads = [threading.Thread(target=worker) for _ in range(8)]\\n        for t in threads:\\n            t.start()\\n\\n        for t in threads:\\n            t.join()\\n        return list(visited)           \\n```"
                    },
                    {
                        "username": "lion2012",
                        "content": "I could not understand the need of input vairable  edges = [[2,0],[2,1],[3,2],[3,1],[0,4]] . \\n\\n\\n\\n![image](https://assets.leetcode.com/users/images/93608c17-d51f-4e25-9fd3-f988de38eac7_1618666015.6092336.png)\\n\\n\\n\\n\\nWhat does edge [2.0] even means in the above image?\\n\\nCan someone plesae explain me its role in the solution. \\n\\nThanks"
                    }
                ]
            },
            {
                "id": 1775011,
                "content": [
                    {
                        "username": "mshlis",
                        "content": "Follow up:\\n\\n1) Assume we have 10,000 nodes and 1 billion URLs to crawl. We will deploy the same software onto each node. The software can know about all the nodes. We have to minimize communication between machines and make sure each node does equal amount of work. How would your web crawler design change?\\n\\n2) What if one node fails or does not work?\\n\\n3) How do you know when the crawler is done?"
                    },
                    {
                        "username": "Elimathew72",
                        "content": "Shoudn\\'t \"http://psn.wlyby.edu/upkr\" and \"http://psn.wlyby.edu/ubmr be under the same host ?"
                    },
                    {
                        "username": "edaengineer",
                        "content": "All other concurrency questions are much easier compared to this. Why is it not HARD?"
                    },
                    {
                        "username": "richardtang",
                        "content": "Not sure why this solution is always time limited exceeded on 18/20\\n ```\\n# \"\"\"\\n# This is HtmlParser\\'s API interface.\\n# You should not implement it, or speculate about its implementation\\n# \"\"\"\\n#class HtmlParser(object):\\n#    def getUrls(self, url):\\n#        \"\"\"\\n#        :type url: str\\n#        :rtype List[str]\\n#        \"\"\"\\nimport threading\\nfrom collections import deque\\n\\nclass Solution:\\n    def crawl(self, startUrl: str, htmlParser: \\'HtmlParser\\') -> List[str]:\\n        q = deque([startUrl])\\n        visited = {startUrl}\\n\\n        def hostname(url):\\n            return url.split(\\'/\\')[2]\\n\\n        name = hostname(startUrl)\\n        lock = threading.Lock()\\n\\n        def worker():\\n            while q:\\n                    lock.acquire()\\n                    url = q.popleft()\\n                    lock.release()\\n                    urls = htmlParser.getUrls(url)\\n                    for u in urls:\\n                        if u not in visited and hostname(u)==name:\\n                            lock.acquire()\\n                            if u not in visited:\\n                                q.append(u)\\n                                visited.add(u)\\n                            lock.release()\\n\\n        threads = [threading.Thread(target=worker) for _ in range(8)]\\n        for t in threads:\\n            t.start()\\n\\n        for t in threads:\\n            t.join()\\n        return list(visited)           \\n```"
                    },
                    {
                        "username": "lion2012",
                        "content": "I could not understand the need of input vairable  edges = [[2,0],[2,1],[3,2],[3,1],[0,4]] . \\n\\n\\n\\n![image](https://assets.leetcode.com/users/images/93608c17-d51f-4e25-9fd3-f988de38eac7_1618666015.6092336.png)\\n\\n\\n\\n\\nWhat does edge [2.0] even means in the above image?\\n\\nCan someone plesae explain me its role in the solution. \\n\\nThanks"
                    }
                ]
            },
            {
                "id": 1575146,
                "content": [
                    {
                        "username": "mshlis",
                        "content": "Follow up:\\n\\n1) Assume we have 10,000 nodes and 1 billion URLs to crawl. We will deploy the same software onto each node. The software can know about all the nodes. We have to minimize communication between machines and make sure each node does equal amount of work. How would your web crawler design change?\\n\\n2) What if one node fails or does not work?\\n\\n3) How do you know when the crawler is done?"
                    },
                    {
                        "username": "Elimathew72",
                        "content": "Shoudn\\'t \"http://psn.wlyby.edu/upkr\" and \"http://psn.wlyby.edu/ubmr be under the same host ?"
                    },
                    {
                        "username": "edaengineer",
                        "content": "All other concurrency questions are much easier compared to this. Why is it not HARD?"
                    },
                    {
                        "username": "richardtang",
                        "content": "Not sure why this solution is always time limited exceeded on 18/20\\n ```\\n# \"\"\"\\n# This is HtmlParser\\'s API interface.\\n# You should not implement it, or speculate about its implementation\\n# \"\"\"\\n#class HtmlParser(object):\\n#    def getUrls(self, url):\\n#        \"\"\"\\n#        :type url: str\\n#        :rtype List[str]\\n#        \"\"\"\\nimport threading\\nfrom collections import deque\\n\\nclass Solution:\\n    def crawl(self, startUrl: str, htmlParser: \\'HtmlParser\\') -> List[str]:\\n        q = deque([startUrl])\\n        visited = {startUrl}\\n\\n        def hostname(url):\\n            return url.split(\\'/\\')[2]\\n\\n        name = hostname(startUrl)\\n        lock = threading.Lock()\\n\\n        def worker():\\n            while q:\\n                    lock.acquire()\\n                    url = q.popleft()\\n                    lock.release()\\n                    urls = htmlParser.getUrls(url)\\n                    for u in urls:\\n                        if u not in visited and hostname(u)==name:\\n                            lock.acquire()\\n                            if u not in visited:\\n                                q.append(u)\\n                                visited.add(u)\\n                            lock.release()\\n\\n        threads = [threading.Thread(target=worker) for _ in range(8)]\\n        for t in threads:\\n            t.start()\\n\\n        for t in threads:\\n            t.join()\\n        return list(visited)           \\n```"
                    },
                    {
                        "username": "lion2012",
                        "content": "I could not understand the need of input vairable  edges = [[2,0],[2,1],[3,2],[3,1],[0,4]] . \\n\\n\\n\\n![image](https://assets.leetcode.com/users/images/93608c17-d51f-4e25-9fd3-f988de38eac7_1618666015.6092336.png)\\n\\n\\n\\n\\nWhat does edge [2.0] even means in the above image?\\n\\nCan someone plesae explain me its role in the solution. \\n\\nThanks"
                    }
                ]
            }
        ]
    }
]